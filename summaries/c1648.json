{"id":"c1648","number":2916,"headline":{"en":"Word Embedding Advancements","da":"Fremskridt inden for ordindlejring"},"content":{"en":"Word Embedding Advancements: Neural machine translation models are better at learning word representations than established algorithms. A toolbox called PSDVec implements a blockwise online learning algorithm for word embeddings. Cross-lingual word embeddings are used to align basic meaning-bearing units of different languages. Word embeddings can be learned from word association norms using the node2vec algorithm. Algorithms like Flexible Lexical Chain II and Fixed Lexical Chain II enhance word embeddings through multi-semantic representation. Word embeddings based on large-scale web corpora are used as a lexicographic tool in the Aranea Project. A supervised algorithm produces task-optimized word embeddings for text classification representations. Word2vec embeddings can represent the context of collocations for distinguishing lexical functions in a Spanish corpus. An additive modification to the objective function encourages interpretability in word embeddings while preserving semantic structure. Semantic information from WordNet is augmented with word2vec embeddings for text classification. Word embeddings can be trained from subtitles in multiple languages using the OpenSubtitles corpus. Deep learning and topological metrics are used to generate cross-lingual word vectors for low-resourced languages. Word2vec and fastText methods are adapted to learn Welsh word embeddings considering syntactic and morphological idiosyncrasies. Word2vec and fastText embeddings are aligned using cosine similarity, inverted softmax, and CSLS metrics for the English-Welsh language pair. Morphological skip-gram replaces fastText characters n-gram with morphological knowledge in word embeddings. Different learning methods and variables impact the quality of cross-lingual word embeddings.","da":"Fremskridt inden for ordindlejring: Neurale maskinoversættelsesmodeller er bedre til at lære ordrepræsentationer end etablerede algoritmer. Et værktøjskasse kaldet PSDVec implementerer en blokvis online læringsalgoritme for ordindlejringer. Kryds-linguale ordindlejringer bruges til at tilpasse grundlæggende betydningsbærende enheder i forskellige sprog. Ordindlejringer kan læres fra ordassociationer ved at bruge node2vec-algoritmen. Algoritmer som Flexible Lexical Chain II og Fixed Lexical Chain II forbedrer ordindlejringer gennem multi-semantisk repræsentation. Ordindlejringer baseret på store web-korpora bruges som et leksikografisk værktøj i Aranea-projektet. En overvåget algoritme producerer opgaveoptimerede ordindlejringer til tekstklassifikationsrepræsentationer. Word2vec-indlejringer kan repræsentere konteksten af kollokationer for at skelne leksikalske funktioner i en spansk korpus. En additiv ændring til målfunktionen fremmer fortolkelighed i ordindlejringer, mens den bevarer semantisk struktur. Semantisk information fra WordNet forstærkes med word2vec-indlejringer til tekstklassifikation. Ordindlejringer kan trænes fra undertekster på flere sprog ved hjælp af OpenSubtitles-korpuset. Dyb læring og topologiske metrikker bruges til at generere kryds-linguale ordvektorer for sprog med begrænsede ressourcer. Word2vec- og fastText-metoder tilpasses for at lære walisiske ordindlejringer under hensyntagen til syntaktiske og morfologiske særpræg. Word2vec- og fastText-indlejringer er tilpasset ved hjælp af kosinussimilitud, inverteret softmax og CSLS-metrikker for det engelsk-walisiske sprogpar. Morfologisk skip-gram erstatter fastText-karakter n-gram med morfologisk viden i ordindlejringer. Forskellige læringsmetoder og variabler påvirker kvaliteten af kryds-linguale ordindlejringer."},"bots":{"critic":{"en":null,"da":null},"potential":{"en":null,"da":null}}}