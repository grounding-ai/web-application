{"id":"c2359","number":2676,"headline":{"en":"Continual learning advancements","da":"Fortsatte fremskridt inden for læring"},"content":{"en":"Continual learning advancements: Researchers are exploring ways to overcome the problem of catastrophic forgetting in artificial neural networks, where the networks lose previously learned knowledge when learning new tasks. They are developing algorithms that selectively slow down learning on important weights for old tasks, use curriculum and progression procedures to grow the model's capacity, encode sets of network weights using autoencoders, approximate online Bayes updates, prevent neurons from embracing distinct concepts, partition the learned space into different areas, combine self-supervised learning signals with incremental learning methods, dynamically expand the neural network structure, use long-term and short-term forgetting mechanisms, preserve weights learned from previous tasks, use neural architecture search to find concise architectures, constrain the evolution of embedding features, transfer knowledge from pre-trained models, assign subnetworks to each task, combine lessons from open set recognition and active learning, add new layers to neural networks, create information processing paths through the network, store short-term and long-term memories separately, relax vicinity constraints, and use bio-inspired learning algorithms based on sparse representations and lateral recurrent connections. These algorithms aim to improve the accuracy and efficiency of continual learning, where neural networks can learn new tasks without forgetting previously acquired knowledge.","da":"Fortsatte fremskridt inden for læring: Forskere udforsker måder at overvinde problemet med katastrofal glemning i kunstige neurale netværk, hvor netværkene mister tidligere indlært viden, når de lærer nye opgaver. De udvikler algoritmer, der selektivt nedsætter indlæring på vigtige vægte for gamle opgaver, bruger pensum- og progressionstrin for at øge modellens kapacitet, koder sæt af netværksvægte ved hjælp af autoencodere, tilnærmer online Bayes-opdateringer, forhindrer neuroner i at favne forskellige koncepter, opdeler det indlærte rum i forskellige områder, kombinerer selv-superviserede læringssignaler med inkrementelle læringsmetoder, udvider dynamisk den neurale netværksstruktur, bruger langtid- og korttidsglemningsmekanismer, bevarer vægte indlært fra tidligere opgaver, bruger neural arkitektursøgning for at finde præcise arkitekturer, begrænser evolutionen af indlejringsfunktioner, overfører viden fra præ-trænede modeller, tildeler subnetværk til hver opgave, kombinerer lærdomme fra open set-genkendelse og aktiv læring, tilføjer nye lag til neurale netværk, skaber informationsbehandlingsveje gennem netværket, opbevarer korttids- og langtidsminder separat, slapper af nærhedskrav og bruger bio-inspirerede læringsalgoritmer baseret på sparsomme repræsentationer og laterale tilbagevendende forbindelser. Disse algoritmer sigter mod at forbedre nøjagtigheden og effektiviteten af kontinuerlig læring, hvor neurale netværk kan lære nye opgaver uden at glemme tidligere erhvervet viden."},"bots":{"critic":{"en":null,"da":null},"potential":{"en":null,"da":null}}}